Image Classification + OOP (MNIST)
File structure:
ğŸ“Œ Project Overview â€“ a general description of what the project is and what it is for.
ğŸ›  Technologies Used â€“ list of technologies and libraries.
ğŸ“ Project Structure â€“ explanation of folder structure.
ğŸš€ Installation & Setup â€“ a detailed guide on how to launch a project.
ğŸ¯ How the Solution Works â€“ explanation of how the code works.
ğŸ” Testing & Edge Cases â€“ edge case testing results.
ğŸ“Š Results & Analysis â€“ conclusions about the accuracy of the models.

ğŸ“Œ Project Overview

This project implements three classification models for the MNIST dataset:
1ï¸âƒ£ Random Forest (rf) â€“ a non-neural machine learning model.
2ï¸âƒ£ Feed-Forward Neural Network (NN) (nn) â€“ a basic deep learning model.
3ï¸âƒ£ Convolutional Neural Network (CNN) (cnn) â€“ the most advanced model.

Each model follows Object-Oriented Programming (OOP) principles and implements a common interface (MnistClassifierInterface).
A wrapper class (MnistClassifier) allows seamless switching between models.

The project includes:
âœ” A structured Python implementation of MNIST classification models.
âœ” Object-oriented design to maintain modularity.
âœ” Jupyter Notebook (demo.ipynb) showcasing model performance and edge cases.
âœ” Edge case testing for robustness evaluation.

ğŸ›  Technologies Used

Python 3.10

TensorFlow 2.13.0 (Neural Networks & CNNs)

Scikit-learn 1.3.0 (Random Forest Classifier)

NumPy 1.24.3 (Data manipulation)

Matplotlib 3.7.1 (Visualizations)

Seaborn 0.12.2 (Advanced statistical plots)

OpenCV 4.8.0.76 (Edge case image modifications)

ğŸ“ Project Structure

ğŸ“‚ Image_classification_OOP
 â”œâ”€â”€ ğŸ“‚ Winstars AI DS internship test
 â”‚   â”œâ”€â”€ ğŸ“‚ models
 â”‚   â”‚   â”œâ”€â”€ mnist_classifier_interface.py  # Interface for models
 â”‚   â”‚   â”œâ”€â”€ random_forest_classifier.py    # Random Forest implementation
 â”‚   â”‚   â”œâ”€â”€ neural_network_classifier.py   # Neural Network implementation
 â”‚   â”‚   â”œâ”€â”€ cnn_classifier.py              # CNN implementation
 â”‚   â”‚   â”œâ”€â”€ mnist_classifier.py            # Wrapper for all models
 â”‚   â”‚   â”œâ”€â”€ __init__.py
 â”‚   â”œâ”€â”€ ğŸ“‚ notebooks
 â”‚   â”‚   â”œâ”€â”€ demo.ipynb  # Jupyter Notebook with full testing & visualization
 â”‚   â”‚   â”œâ”€â”€ draft.ipynb  # Draft Notebook where I tested the code.
 â”‚   â”œâ”€â”€ requirements.txt  # Required libraries
 â”‚   â”œâ”€â”€ README.md  # Project documentation

ğŸš€ Installation & Setup

1ï¸âƒ£ Clone the repository

git clone https://github.com/Markol06/AI-DS-testproject.git
cd AI-DS-testproject/Image_classification_OOP/Winstars AI DS internship test

2ï¸âƒ£ Install dependencies

Run the following command to install all required libraries:

pip install -r requirements.txt

3ï¸âƒ£ Run the demo

Option 1: Run in Jupyter Notebook

jupyter notebook

Then open notebooks/demo.ipynb and run all cells.

Option 2: Run directly in Python

python models/mnist_classifier.py

The script will train and test models automatically.

ğŸ¯ How the Solution Works

1ï¸âƒ£ Model Architecture

Each model follows an interface (MnistClassifierInterface) that defines:

train(X_train, y_train): Method to train the model.

predict(X_test): Method to make predictions.

Models implemented:

Random Forest (RF) â€“ traditional ML classifier, works with flattened images.

Neural Network (NN) â€“ basic deep learning model with dense layers.

CNN â€“ convolutional layers extract spatial features for improved accuracy.

2ï¸âƒ£ Model Selection via MnistClassifier

To use a specific model, simply initialize:

classifier = MnistClassifier("cnn")  # Options: "cnn", "nn", "rf"
classifier.train()
prediction = classifier.predict(X_test[0])

ğŸ” Testing & Edge Cases

The project tests models on challenging cases:

Blurred digits: CNN and NN performed best, while RF failed.

Cropped digits: All models recognized cropped digits correctly.

Resized digits: CNN handled scaling well, while RF and NN struggled.

Skewed digits: All models had difficulty with skewed inputs.

Noisy digits: All models classified noisy digits correctly.

Darkened digits: Even minor darkening caused misclassification in all models.

Brightened digits: Overexposed digits led to incorrect results.

ğŸ“Š Results & Analysis

Random Forest achieved ~97.0% accuracy in 32 seconds.

Neural Network achieved ~97.8% accuracy in 20 seconds.

CNN achieved ~99.2% accuracy, but took 515 seconds to train.

Key Findings:
CNN provides the best accuracy but has the longest training time. Random Forest is the fastest but less accurate. All models struggle with darkened and brightened digits.

ğŸ‘¨â€ğŸ’» Author & Contributions

Project created by Marko as part of the AI-DS internship test.


